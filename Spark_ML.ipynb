{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veranika-izrailyan/Machine_Learning/blob/main/spark_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQo6poQ5x7PT"
      },
      "source": [
        "# Pyspark - Machine Learning Pipelines\n",
        "\n",
        "#### Description\n",
        "In this Hands-on lab, you will master your knowledge on PySpark, a very popular Python library for big data analysis and modeling. Here, you will learn how to create a machine learning pipeline using the PySpark library, and to perform metric evaluation and model tuning.\n",
        "\n",
        "Your machine learning skills will be challenged, and by the end of this lab, you should have a deep understanding of how PySpark practically works to build data analysis pipelines.\n",
        "\n",
        "\n",
        "\n",
        "#### Learning Objectives\n",
        "Upon completion of this lab you will be able to:\n",
        " - fit a Logistic Regression model in PySpark;\n",
        " - perform cross-validation in PySPark;\n",
        " - evaluate the model performances;\n",
        " - perform inference on new, unseen data.\n",
        "\n",
        "\n",
        "#### Intended Audience\n",
        "This lab is intended for:\n",
        " - Those interested in performing data analysis with Python.\n",
        " - Anyone involved in data science and engineering pipelines.\n",
        "\n",
        "#### Prerequisites\n",
        "You should possess:\n",
        " - An intermediate understanding of Python.\n",
        " - Basic knowledge of SQL.\n",
        " - Basic knowledge of the following libraries: Pandas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgtGKySmBD0W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu6wA4ktexuD"
      },
      "source": [
        "# 1. Dataset Creation\n",
        "We assume you have already completed the `PySpark - Prerocessing` lab. As a consequence, we are not going to dive into the data preprocessing phase here in this hands-on lab: in case you wish to master your skills on data transformation using pyspark, please check the aforementioned Laboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-WgrQLlzmyt"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "file_path = \"data/tips.csv\"\n",
        "tips = spark.read.csv(file_path, header=True)\n",
        "tips.createOrReplaceTempView(\"tips\")\n",
        "tips_table = spark.table(\"tips\")\n",
        "tips_table = tips_table.withColumn(\"perc_tips\", (tips.tip/tips.total_bill)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1TWhPivgV2M"
      },
      "outputs": [],
      "source": [
        "tips_table = tips_table.withColumn(\"total_bill\",tips_table.total_bill.cast(\"double\"))\n",
        "tips_table = tips_table.withColumn(\"tip\", tips_table.tip.cast(\"double\"))\n",
        "tips_table = tips_table.withColumn(\"size\", tips_table.size.cast(\"integer\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ianHHVrf42R",
        "outputId": "547bf95e-362d-4e25-fd8e-36f688ad3ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+------+------+---+------+----+------------------+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|         perc_tips|\n",
            "+----------+----+------+------+---+------+----+------------------+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|5.9446733372572105|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|16.054158607350097|\n",
            "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|16.658733936220845|\n",
            "|     23.68|3.31|  Male|    No|Sun|Dinner|   2| 13.97804054054054|\n",
            "|     24.59|3.61|Female|    No|Sun|Dinner|   4|14.680764538430255|\n",
            "|     25.29|4.71|  Male|    No|Sun|Dinner|   4| 18.62396204033215|\n",
            "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2| 22.80501710376283|\n",
            "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|11.607142857142858|\n",
            "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|13.031914893617023|\n",
            "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|21.853856562922868|\n",
            "|     10.27|1.71|  Male|    No|Sun|Dinner|   2| 16.65043816942551|\n",
            "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|14.180374361883155|\n",
            "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|10.181582360570687|\n",
            "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|16.277807921866522|\n",
            "|     14.83|3.02|Female|    No|Sun|Dinner|   2|20.364126770060686|\n",
            "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|18.164967562557923|\n",
            "|     10.33|1.67|Female|    No|Sun|Dinner|   3| 16.16650532429816|\n",
            "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|22.774708410067525|\n",
            "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|20.624631703005306|\n",
            "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|16.222760290556902|\n",
            "+----------+----+------+------+---+------+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tips_table.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4blq1Qpig3F"
      },
      "source": [
        "## Build a Machine Learning Pipeline with PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWPkpoebgDMM"
      },
      "source": [
        "Assume that our restaurant's clients can be divided in two categories, based on thei generosity. In particular, we can state that one is generous if he/she gives a tip of at least 15 USD, otherwise we call him/her a stingy person. We can easily build a labelled dataset using this information by performing some data transformation with PySpark.\n",
        "\n",
        "To do so, we use the `withColumn` method to create two columns:\n",
        " 1. a binary one called `is_stingy`, which is based on the boolean condition ` tips.perc_tips < 15`;\n",
        " 2. another one called `label` which actually converts the boolean condition into a binary label - 0 if generous, else 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muRX0ugTijwa"
      },
      "outputs": [],
      "source": [
        "tips_table = tips_table.withColumn(\"is_stingy\", tips_table.perc_tips < 15)\n",
        "tips_table = tips_table.withColumn(\"label\", tips_table.is_stingy.cast(\"integer\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pV-ESn7xtc",
        "outputId": "adc8358a-c3da-4280-e7a3-c480045568d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+------+------+---+------+----+------------------+---------+-----+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|         perc_tips|is_stingy|label|\n",
            "+----------+----+------+------+---+------+----+------------------+---------+-----+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|5.9446733372572105|     true|    1|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|16.054158607350097|    false|    0|\n",
            "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|16.658733936220845|    false|    0|\n",
            "|     23.68|3.31|  Male|    No|Sun|Dinner|   2| 13.97804054054054|     true|    1|\n",
            "|     24.59|3.61|Female|    No|Sun|Dinner|   4|14.680764538430255|     true|    1|\n",
            "|     25.29|4.71|  Male|    No|Sun|Dinner|   4| 18.62396204033215|    false|    0|\n",
            "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2| 22.80501710376283|    false|    0|\n",
            "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|11.607142857142858|     true|    1|\n",
            "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|13.031914893617023|     true|    1|\n",
            "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|21.853856562922868|    false|    0|\n",
            "|     10.27|1.71|  Male|    No|Sun|Dinner|   2| 16.65043816942551|    false|    0|\n",
            "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|14.180374361883155|     true|    1|\n",
            "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|10.181582360570687|     true|    1|\n",
            "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|16.277807921866522|    false|    0|\n",
            "|     14.83|3.02|Female|    No|Sun|Dinner|   2|20.364126770060686|    false|    0|\n",
            "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|18.164967562557923|    false|    0|\n",
            "|     10.33|1.67|Female|    No|Sun|Dinner|   3| 16.16650532429816|    false|    0|\n",
            "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|22.774708410067525|    false|    0|\n",
            "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|20.624631703005306|    false|    0|\n",
            "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|16.222760290556902|    false|    0|\n",
            "+----------+----+------+------+---+------+----+------------------+---------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tips_table.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSVhKl9OhH-y"
      },
      "source": [
        "Let's check the distribution of the label column with respect to the gender:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_FVtmSEmZCy",
        "outputId": "85313da9-2a67-4684-990c-7906d0506372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+-----+\n",
            "|label|   sex|count|\n",
            "+-----+------+-----+\n",
            "|    1|Female|   34|\n",
            "|    0|  Male|   83|\n",
            "|    1|  Male|   74|\n",
            "|    0|Female|   53|\n",
            "+-----+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "by_label_sex = tips_table.groupBy(\"label\", \"sex\")\n",
        "by_label_sex.count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrfFhWVWupix"
      },
      "source": [
        "### Encode categorical variables using PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsz5DnMH7Xdt"
      },
      "source": [
        "When fitting a Machine Learning model we need to perform a few steps of data manipulation and transformation, such as converting categorical features into numerical ones. To do so, we can employ the PySpark `StringIndexer` function. The `StringIndexer` allows to encode a categorical feature into a numerical one. It starts by assigning the lowest rank - i.e. zero - to the most frequent value of that column. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Crd5VfmrSbB"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1EmGzQV95gi"
      },
      "source": [
        "We here apply the `StringIndexer` to the `sex` column as follows: we pass the raw column `sex`, and we specify the output column as `sex_index`. We then apply a fit and transform method on the `tips_table` to convert the `sex` column into a categorical one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj6Z-PtylQap"
      },
      "outputs": [],
      "source": [
        "sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"sex_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aWtx0JDlRR5",
        "outputId": "d6125bf3-0f58-4c0d-9e29-24420e2b7727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+------+------+---+------+----+------------------+---------+-----+---------+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|         perc_tips|is_stingy|label|sex_index|\n",
            "+----------+----+------+------+---+------+----+------------------+---------+-----+---------+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|5.9446733372572105|     true|    1|      1.0|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|16.054158607350097|    false|    0|      0.0|\n",
            "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|16.658733936220845|    false|    0|      0.0|\n",
            "|     23.68|3.31|  Male|    No|Sun|Dinner|   2| 13.97804054054054|     true|    1|      0.0|\n",
            "|     24.59|3.61|Female|    No|Sun|Dinner|   4|14.680764538430255|     true|    1|      1.0|\n",
            "|     25.29|4.71|  Male|    No|Sun|Dinner|   4| 18.62396204033215|    false|    0|      0.0|\n",
            "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2| 22.80501710376283|    false|    0|      0.0|\n",
            "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|11.607142857142858|     true|    1|      0.0|\n",
            "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|13.031914893617023|     true|    1|      0.0|\n",
            "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|21.853856562922868|    false|    0|      0.0|\n",
            "|     10.27|1.71|  Male|    No|Sun|Dinner|   2| 16.65043816942551|    false|    0|      0.0|\n",
            "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|14.180374361883155|     true|    1|      1.0|\n",
            "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|10.181582360570687|     true|    1|      0.0|\n",
            "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|16.277807921866522|    false|    0|      0.0|\n",
            "|     14.83|3.02|Female|    No|Sun|Dinner|   2|20.364126770060686|    false|    0|      1.0|\n",
            "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|18.164967562557923|    false|    0|      0.0|\n",
            "|     10.33|1.67|Female|    No|Sun|Dinner|   3| 16.16650532429816|    false|    0|      1.0|\n",
            "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|22.774708410067525|    false|    0|      0.0|\n",
            "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|20.624631703005306|    false|    0|      1.0|\n",
            "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|16.222760290556902|    false|    0|      0.0|\n",
            "+----------+----+------+------+---+------+----+------------------+---------+-----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = sex_indexer.fit(tips_table).transform(tips_table)\n",
        "df1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QovkAvbgmEl5"
      },
      "source": [
        "Please note that the value `\"Male\"` gets index 0 because it is the most frequent, and we can easily check this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPqjCiaslooZ",
        "outputId": "03a68465-aa35-4fe1-ef83-796e33ac873e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----+\n",
            "|   sex|count|\n",
            "+------+-----+\n",
            "|Female|   87|\n",
            "|  Male|  157|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tips_table.groupBy(\"sex\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYZWyW_vD3ZP"
      },
      "source": [
        "We now apply to `sex`, `smoker`, `day` and `time` both the `StringIndexer` and the `OneHotEncoder`: while the former has been discussed just a few lines above, the latter deserves a few more words. In a nutshell, this transformer class creates, in a new Spark DaatFrame, a dummy variable for each single value observed in the column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6zuP2gd17VH"
      },
      "source": [
        "For example with 4 categories, an input value of `2.0` would map to an output vector of `[0.0, 0.0, 1.0]`. The last category is not included by default (configurable via the argument `dropLast`), because it makes the vector entries sum up to one, and hence linearly dependent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv2E7pqw1gqI"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsCS9wBjuF41"
      },
      "outputs": [],
      "source": [
        "# sex\n",
        "sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"sex_index\")\n",
        "sex_encoder = OneHotEncoder(inputCol=\"sex_index\", outputCol=\"sex_fact\")\n",
        "\n",
        "# smoker\n",
        "smoker_indexer = StringIndexer(inputCol=\"smoker\", outputCol=\"smoker_index\")\n",
        "smoker_encoder = OneHotEncoder(inputCol=\"smoker_index\", outputCol=\"smoker_fact\")\n",
        "\n",
        "# day\n",
        "day_indexer = StringIndexer(inputCol=\"day\", outputCol=\"day_index\")\n",
        "day_encoder = OneHotEncoder(inputCol=\"day_index\", outputCol=\"day_fact\")\n",
        "\n",
        "# time\n",
        "time_indexer = StringIndexer(inputCol=\"time\", outputCol=\"time_index\")\n",
        "time_encoder = OneHotEncoder(inputCol=\"time_index\", outputCol=\"time_fact\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CycxBr4XuurZ"
      },
      "source": [
        "### Assembling Columns using VectorAssembler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJs_UOEhHcgt"
      },
      "source": [
        "Now, our goal is to combine the numerical features `[\"total_bill\", \"tip\", \"time_fact\", \"day_fact\", \"smoker_fact\", \"sex_fact\",  \"size\", \"perc_tips\"]` into a single vector column, that we call for simplicity `features`. To do so, we use the VectorAssembler, which is a trnasformer that combines a givn list of columns into a single vector column. VectorAssembler will have two parameters:\n",
        "\n",
        "*   `inputCols`: list of features to combine into a single vector column;\n",
        "*   `outputCol`: the new column that will contain the transformed vector.\n",
        "\n",
        "\n",
        "Let’s create our assembler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azF8t5p7uuIx"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "vec_assembler = VectorAssembler(\n",
        "    inputCols=[\"total_bill\", \"tip\", \"time_fact\", \"day_fact\", \"smoker_fact\", \"sex_fact\",  \"size\", \"perc_tips\"], \n",
        "    outputCol=\"features\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmofJW1i5CSS"
      },
      "source": [
        "Now using this assembler we can transform the raw dataset and take a look as the result. However, we have an issue: in the original dataset, we do not have access to, for instance, the columns `time_index` and `time_fact`, since they come from the StringIndexer and OneHotEncoder, respectively. So we need to somehow combine them using the `Pipeline` object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2Fx2DYGuGab"
      },
      "source": [
        "### Create the Data Pipeline in PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFOMGP4yrpFU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "tips_pipe = Pipeline(\n",
        "    stages=[\n",
        "            sex_indexer, sex_encoder, \n",
        "            smoker_indexer, smoker_encoder, \n",
        "            day_indexer, day_encoder, \n",
        "            time_indexer, time_encoder, \n",
        "            vec_assembler\n",
        "            ]\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvWyqSadwjFe"
      },
      "outputs": [],
      "source": [
        "piped_data = tips_pipe.fit(tips_table).transform(tips_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFBfD2zp6myx"
      },
      "source": [
        "Now using this pipeline we can transform the original dataset and take a look as the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppmOTkP8wqmq",
        "outputId": "53f696cd-ef5e-43fc-f6a6-5b9b26a724d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+------+------+---+------+----+------------------+---------+-----+---------+-------------+------------+-------------+---------+-------------+----------+-------------+--------------------+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|         perc_tips|is_stingy|label|sex_index|     sex_fact|smoker_index|  smoker_fact|day_index|     day_fact|time_index|    time_fact|            features|\n",
            "+----------+----+------+------+---+------+----+------------------+---------+-----+---------+-------------+------------+-------------+---------+-------------+----------+-------------+--------------------+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|5.9446733372572105|     true|    1|      1.0|    (1,[],[])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[16.99,1.01,1.0,0...|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|16.054158607350097|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[10.34,1.66,1.0,0...|\n",
            "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|16.658733936220845|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[21.01,3.5,1.0,0....|\n",
            "|     23.68|3.31|  Male|    No|Sun|Dinner|   2| 13.97804054054054|     true|    1|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[23.68,3.31,1.0,0...|\n",
            "|     24.59|3.61|Female|    No|Sun|Dinner|   4|14.680764538430255|     true|    1|      1.0|    (1,[],[])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[24.59,3.61,1.0,0...|\n",
            "|     25.29|4.71|  Male|    No|Sun|Dinner|   4| 18.62396204033215|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[25.29,4.71,1.0,0...|\n",
            "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2| 22.80501710376283|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[8.77,2.0,1.0,0.0...|\n",
            "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|11.607142857142858|     true|    1|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[26.88,3.12,1.0,0...|\n",
            "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|13.031914893617023|     true|    1|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[15.04,1.96,1.0,0...|\n",
            "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|21.853856562922868|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[14.78,3.23,1.0,0...|\n",
            "|     10.27|1.71|  Male|    No|Sun|Dinner|   2| 16.65043816942551|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[10.27,1.71,1.0,0...|\n",
            "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|14.180374361883155|     true|    1|      1.0|    (1,[],[])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[35.26,5.0,1.0,0....|\n",
            "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|10.181582360570687|     true|    1|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[15.42,1.57,1.0,0...|\n",
            "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|16.277807921866522|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[18.43,3.0,1.0,0....|\n",
            "|     14.83|3.02|Female|    No|Sun|Dinner|   2|20.364126770060686|    false|    0|      1.0|    (1,[],[])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[14.83,3.02,1.0,0...|\n",
            "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|18.164967562557923|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[21.58,3.92,1.0,0...|\n",
            "|     10.33|1.67|Female|    No|Sun|Dinner|   3| 16.16650532429816|    false|    0|      1.0|    (1,[],[])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[10.33,1.67,1.0,0...|\n",
            "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|22.774708410067525|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[16.29,3.71,1.0,0...|\n",
            "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|20.624631703005306|    false|    0|      1.0|    (1,[],[])|         0.0|(1,[0],[1.0])|      1.0|(3,[1],[1.0])|       0.0|(1,[0],[1.0])|[16.97,3.5,1.0,0....|\n",
            "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|16.222760290556902|    false|    0|      0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      0.0|(3,[0],[1.0])|       0.0|(1,[0],[1.0])|[20.65,3.35,1.0,1...|\n",
            "+----------+----+------+------+---+------+----+------------------+---------+-----+---------+-------------+------------+-------------+---------+-------------+----------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "piped_data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8oiPJQj6thl"
      },
      "source": [
        "We see that the last column `features` is the combination of several columns obtained by applying the StringIndexer, OneHotEncoder and VectorAssembler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO0E0Ujky88I"
      },
      "source": [
        "### Classify customers based on their \"generosity\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USFVoNP19j1y"
      },
      "source": [
        "Let us split the data into the standard rule 80-20: 80% of the raw data is used to train a classifier, and the rest to test the model on unseed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IZ1b8Xly7BQ"
      },
      "outputs": [],
      "source": [
        "training, test = piped_data.randomSplit([.8, .2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ4oqIFJ-YVi"
      },
      "source": [
        "We use a Logistic Regression to classify our clients in those two categories.\n",
        "In case you want to master your knowledge on Logistic Regression, please check our content library: we have a few courses where this concept is explained. Among many, you can refer to `Building Machine Learning Pipelines with scikit-learn - Part 2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgR8jiKV2xim"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWdyclGy_SuD"
      },
      "source": [
        "We can fit easily the modelby passing the training data, and then we can check the model performances by looking at the raw data, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjHMAp8s_eaq",
        "outputId": "a1e09a51-0fd1-47c3-a3d1-7db1629f3b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+----------+-----+\n",
            "|            features|       rawPrediction|         probability|prediction|label|\n",
            "+--------------------+--------------------+--------------------+----------+-----+\n",
            "|(10,[0,1,2,8,9],[...|[1227.69183676080...|           [1.0,0.0]|       0.0|    0|\n",
            "|[7.25,1.0,1.0,1.0...|[-550.98960464462...|[5.10804973058571...|       1.0|    1|\n",
            "|[7.25,5.15,1.0,0....|[24383.0224430444...|           [1.0,0.0]|       0.0|    0|\n",
            "|[7.51,2.0,0.0,0.0...|[4959.57156545339...|           [1.0,0.0]|       0.0|    0|\n",
            "|[7.56,1.44,0.0,0....|[1667.34713489663...|           [1.0,0.0]|       0.0|    0|\n",
            "|[7.74,1.44,1.0,1....|[1776.49706574858...|           [1.0,0.0]|       0.0|    0|\n",
            "|[8.35,1.5,0.0,0.0...|[1283.45009513032...|           [1.0,0.0]|       0.0|    0|\n",
            "|[8.51,1.25,0.0,0....|[-161.43668973437...|[7.74348518224750...|       1.0|    1|\n",
            "|[8.52,1.48,0.0,0....|[947.60491743479,...|           [1.0,0.0]|       0.0|    0|\n",
            "|(10,[0,1,7,8,9],[...|[3506.50893513566...|           [1.0,0.0]|       0.0|    0|\n",
            "|[8.77,2.0,1.0,0.0...|[3406.29680011805...|           [1.0,0.0]|       0.0|    0|\n",
            "|[9.55,1.45,1.0,1....|[69.1037906932761...|           [1.0,0.0]|       0.0|    0|\n",
            "|[9.6,4.0,1.0,0.0,...|[12214.9595200814...|           [1.0,0.0]|       0.0|    0|\n",
            "|[9.68,1.32,1.0,0....|[-679.39086305966...|[8.79624095982712...|       1.0|    1|\n",
            "|[9.78,1.73,0.0,0....|[1103.13352151542...|           [1.0,0.0]|       0.0|    0|\n",
            "|[9.94,1.56,1.0,0....|[247.044332050435...|           [1.0,0.0]|       0.0|    0|\n",
            "|[10.07,1.25,1.0,1...|[-1190.9484471463...|           [0.0,1.0]|       1.0|    1|\n",
            "|[10.07,1.83,0.0,0...|[1290.58777749468...|           [1.0,0.0]|       0.0|    0|\n",
            "|(10,[0,1,8,9],[10...|[2605.50824475805...|           [1.0,0.0]|       0.0|    0|\n",
            "|[10.27,1.71,1.0,0...|[681.142882083344...|           [1.0,0.0]|       0.0|    0|\n",
            "+--------------------+--------------------+--------------------+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lr_fit = lr.fit(training)\n",
        "lr_fit.transform(training).select('features', 'rawPrediction', 'probability', 'prediction', 'label').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEIp6VWhAFGv"
      },
      "source": [
        "Qualitatively, we are not doing bad. However, this is just a qualitative evaluation. We need to be much more consistent on both the way we trained the model and the evaluation process. As for the first, model's performances depend on the way the data is splitted. So a way to avoid splitting bias is to perform cross-validation. Also, please keep in mind that any model is characterized by a set of hyperparameters, that typically need to be estimated. We can therefore add a specific grid for a set of hyperparameters, and perform a search to look for the best hyperparameters combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYmmvVrG93fa"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAuZRj2R8P5Q"
      },
      "outputs": [],
      "source": [
        "grid = ParamGridBuilder()\n",
        "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
        "grid = grid.addGrid(lr.elasticNetParam, [0,1]) # used to perform regularization\n",
        "grid = grid.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpXS_nV_E7lS"
      },
      "outputs": [],
      "source": [
        "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m2GyyXkFsXw"
      },
      "source": [
        "We initialize a `CrossValidator` instance by passing the model we wish to use - in our case a Logistic Regression, the hyperparameters we wish to explore, and the metric used to choose the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vplLyE8831h"
      },
      "outputs": [],
      "source": [
        "cv = CrossValidator(estimator=lr,\n",
        "               estimatorParamMaps=grid,\n",
        "               evaluator=evaluator\n",
        "               )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wauShMlfyepV"
      },
      "source": [
        "The following snippet might take a while, since we need to perform grid search cross validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEGgYjdc976b"
      },
      "outputs": [],
      "source": [
        "models = cv.fit(training)\n",
        "best_lr = models.bestModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtZ3MFfgGFaD"
      },
      "source": [
        "We can easily access to the best model, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cnDX4Hh91SU",
        "outputId": "d6ae4e24-3a64-4e55-c8a0-0c7ccb83f8d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegressionModel: uid=LogisticRegression_ea46406b0a92, numClasses=2, numFeatures=10\n"
          ]
        }
      ],
      "source": [
        "print(best_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHtVl4rKi0YZ"
      },
      "source": [
        "### Testing the model on unseen data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKcj2m48GOI0"
      },
      "source": [
        "Let us evaluate the model on new data: we pick the best model, we transform the raw test data with respect to that model, and we evaluate the performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQgQSxje923g"
      },
      "outputs": [],
      "source": [
        "test_results = best_lr.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgYpnxjljA3A",
        "outputId": "08e9a7b8-d98d-47a4-8d65-ab1fae45ae19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+----------+-----+\n",
            "|            features|       rawPrediction|         probability|prediction|label|\n",
            "+--------------------+--------------------+--------------------+----------+-----+\n",
            "|[3.07,1.0,1.0,1.0...|[16.7379200648088...|[0.99999994619611...|       0.0|    0|\n",
            "|[10.29,2.6,1.0,0....|[9.71900369344847...|[0.99993987373687...|       0.0|    0|\n",
            "|[10.33,1.67,1.0,0...|[1.22517766467732...|[0.77297343848383...|       0.0|    0|\n",
            "|[10.77,1.47,1.0,1...|[-1.1085218986669...|[0.24814655502182...|       1.0|    1|\n",
            "|[11.02,1.98,1.0,1...|[3.12190993862046...|[0.95778751565675...|       0.0|    0|\n",
            "|[11.59,1.5,1.0,1....|[-1.5681096705846...|[0.17248604013624...|       1.0|    1|\n",
            "|[11.69,2.31,0.0,0...|[4.59536608620883...|[0.99000243744303...|       0.0|    0|\n",
            "|[12.43,1.8,0.0,0....|[-0.3478353565796...|[0.41390743970723...|       1.0|    1|\n",
            "|[12.66,2.5,1.0,0....|[4.58300563577699...|[0.98987935460645...|       0.0|    0|\n",
            "|(10,[0,1,5,8,9],[...|[1.06183055657331...|[0.74304021160217...|       0.0|    0|\n",
            "|[13.03,2.0,0.0,0....|[0.47826483529780...|[0.61733805726452...|       0.0|    0|\n",
            "|[13.16,2.75,0.0,0...|[5.63988041454521...|[0.99645928788563...|       0.0|    0|\n",
            "|[13.28,2.72,1.0,1...|[5.26870215534469...|[0.99487609906404...|       0.0|    0|\n",
            "|[13.51,2.0,0.0,0....|[0.16939594512337...|[0.54224800906011...|       0.0|    1|\n",
            "|[13.81,2.0,1.0,0....|[-0.3308551844358...|[0.41803255817185...|       1.0|    1|\n",
            "|[13.81,2.0,1.0,1....|[-0.1307473634551...|[0.46735964445468...|       1.0|    1|\n",
            "|[14.15,2.0,0.0,0....|[-0.6715249292888...|[0.33815546775766...|       1.0|    1|\n",
            "|[15.36,1.64,1.0,1...|[-3.6821603643067...|[0.02455063902823...|       1.0|    1|\n",
            "|[15.48,2.02,0.0,0...|[-1.4683349194195...|[0.18719582994682...|       1.0|    1|\n",
            "|[15.53,3.0,1.0,1....|[4.38199006362687...|[0.98765387505929...|       0.0|    0|\n",
            "+--------------------+--------------------+--------------------+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_results.select('features', 'rawPrediction', 'probability', 'prediction', 'label').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCG-P45S-3zE",
        "outputId": "931fc162-093c-4d64-ec70-a00ee098ab39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Area Under Receiver Operating Characteristic: 1.0\n"
          ]
        }
      ],
      "source": [
        "metric_test = evaluator.evaluate(test_results)\n",
        "print(f'Area Under Receiver Operating Characteristic: {metric_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Z2MZzXGdP0"
      },
      "source": [
        "We are perfectly predicting the test labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BQnoOwZHS1M"
      },
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# Validation Check\n",
        "# DO NOT CHANGE THIS CELL\n",
        "# ====================================\n",
        "with open('results/vcf_01.txt', 'w') as f:\n",
        "    f.write(\"%s\\n\" % metric_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keCx4_n1G2U1"
      },
      "source": [
        "**End Laboratory**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "spark_ml.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
